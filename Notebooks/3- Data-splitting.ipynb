{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02e57c65-796b-4036-b0ce-cd0895285e0f",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Machine Learningâ€”Splitting your Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e644feb1",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/datacamp/workspace-tutorial-python-data-preprocessing-train-test-split/blob/master/notebook-solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ec7c1b9-8ec6-48c8-84a8-668bb3ccba60",
   "metadata": {
    "id": "bA5ajAmk7XH6"
   },
   "source": [
    "## Why do you need to split your dataset into training and testing sets?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aeb88fd2-25ac-4e6d-9d35-be1f7ed525cb",
   "metadata": {},
   "source": [
    "If you train your machine learning model on the whole dataset, you have two problems.\n",
    "\n",
    "Firstly, you don't know how your model will perform on other datasets, because you haven't tested the model on anything else.\n",
    "\n",
    "Secondly, you risk **overfitting** the model. That is, making it work well for one dataset, at the cost of model performance on other datasets. That is, your model will perform worse than it could."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5af2e761-88dc-44db-93e6-b1a2a3a5e6f1",
   "metadata": {},
   "source": [
    "## When do you need to split your dataset?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "83f13e77-4adc-43f7-b9ea-12b6de7d74e1",
   "metadata": {},
   "source": [
    "Splitting the data into training and testing steps should happen **before feature engineering**. Otherwise, you can end up with **data leakage**, where information from the testing set is contained in the training set. This is a type of \"cheating\" which will increase the apparent performance of the model and give you a false sense that the model is better at predicting than it really is. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3916dc80-23ed-486e-8932-90b9dc446618",
   "metadata": {},
   "source": [
    "## Case study: Classifying loan applications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61535c42-fc4d-47be-8c01-17837f5d5050",
   "metadata": {},
   "source": [
    "Let's make predictions on some loan application data. If you want to try your own analysis, you can access this via [its Workspace template](https://app.datacamp.com/workspace/datasets/dataset-python-loans)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6783495c-7297-4b70-aa7d-54a4d32b1560",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84af07e0-5ce2-4623-8ad6-7f052b3236d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_applications = pd.read_csv(\"loan_data.csv\", nrows=1000)\n",
    "loan_applications"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "285aa017-81d3-4f03-aa2a-cac6eab1909f",
   "metadata": {},
   "source": [
    "The response variable is `credit.policy`. It takes the value `1` when the application meets the underwriting policy (so a loan can be issued), and `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c9ea99-642a-452a-8a51-80b8fff7139d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = loan_applications[\"credit.policy\"]\n",
    "response"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9de3d44-05b2-4d16-be41-214ca3d7a696",
   "metadata": {},
   "source": [
    "All the other columns can be used for features.\n",
    "\n",
    "Note the use of [`get_dummies()`](https://pandas.pydata.org/docs/reference/api/pandas.get_dummies.html) to convert the categorical column, `purpose`, to several columns of ones and zeros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55770f27-3a8d-4db5-9938-1cb6223db55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.get_dummies(\n",
    "    loan_applications.drop(columns=\"credit.policy\")\n",
    ")\n",
    "features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c25e277-45e8-4b2b-90c8-956105adba56",
   "metadata": {},
   "source": [
    "## Splitting into training and testing sets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "46665f85-65c9-4884-8eb6-21cc5a4aad28",
   "metadata": {},
   "source": [
    "To split into training and tesing sets, we call [`train_test_split()`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), passing the response and the features.\n",
    "\n",
    "It returns a list of 4 things: \n",
    "\n",
    "- the responses for the training set\n",
    "- the features for the training set\n",
    "- the responses for the testing set\n",
    "- the features for the testing set\n",
    "\n",
    "To make the code easier to work with, we use **variable unpacking** to return 4 separate variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be04b12d-d90d-4abc-bf40-6e9ca8b2be62",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_train, response_test, features_train, features_test = train_test_split(response, features)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "04749ad5-a279-4bbf-aad0-c7d0215e9f80",
   "metadata": {},
   "source": [
    "Looking at the **shape** of each of these variables makes it clear what each contains."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f16205-9cbf-4b33-8d3d-b7d2a0ed6ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_train.shape)\n",
    "print(response_test.shape)\n",
    "print(features_train.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8761067a-218e-4f16-a02c-44aa4da74256",
   "metadata": {},
   "source": [
    "By default, 70% of the data ends up in the training set, and 30% ends up in the testing set."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "567ac727-c4b7-4ab4-9cd1-c4f8224a5a0f",
   "metadata": {},
   "source": [
    "## Controlling the train/test split quantities"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "269c326b-2fe1-4f31-bcdd-f49f5d487b22",
   "metadata": {},
   "source": [
    "To change the amount of data used in the test set, set the `test_size` argument. For very small datasets, it is common to reduce the fraction in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99efd7d0-0838-48f2-a045-a08022c9b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_train, features_train, response_test, features_test = train_test_split(response, features, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7513f-a873-4a56-a50d-66372be1b038",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response_train.shape)\n",
    "print(response_test.shape)\n",
    "print(features_train.shape)\n",
    "print(features_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08d64c98-9076-42d4-baf9-3a88782d8490",
   "metadata": {},
   "source": [
    "## Enforcing reproducibility"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "969b825d-751c-45c3-81a1-7d90383f779a",
   "metadata": {},
   "source": [
    "The training and testing sets are randomly generated. If you want to return exactly the same training and testing sets when you run your code repeatedly (for example when publishing the results in a report), you need to set the random seed with the `random_state` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bb7ec9-59c0-4421-9654-6362a0749f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_train, response_test, features_train, features_test = train_test_split(response, features, random_state=2022)\n",
    "response_train2, response_test2, features_train2, features_test2 = train_test_split(response, features, random_state=2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a0586-9732-472b-bb08-95a36a0c9d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_train.equals(response_train2)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataCamp Workspace",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
