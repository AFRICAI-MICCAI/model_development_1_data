{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The contents of this notebook are redistributed and modified from [TorchIO tutorials](https://github.com/fepegar/torchio/tree/main/tutorials) (Transforms) as per the [Apache-2.0 license](https://github.com/fepegar/torchio/blob/main/LICENSE)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HeAyaKW6Ieu"
      },
      "source": [
        "# Data preprocessing and augmentation using TorchIO: a tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cQDWFrRov7C"
      },
      "source": [
        "Preprocessing and augmentation are always overlooked in papers, but these are extremely important and therefore the software and parameters used should be correctly reported. Most importantly, papers will be more easily reproducible if researchers can share the tools they use for these tasks.\n",
        "\n",
        "![One does not simply throw data into a U-Net](https://i.imgflip.com/4b1gac.jpg)\n",
        "\n",
        "Transforms are callable Python objects that take data and modify it. In the context of deep learning for medical images, they can be used to pre- or post-process the images, or for data augmentation.\n",
        "\n",
        "[TorchIO](https://torchio.readthedocs.io/index.html) transforms support 4D tensors (and therefore also 2D and 3D), so it can be used to process most types of medical images: X-rays, CT, 4D ultrasound, structural MRI, diffusion MRI, functional MRI, PET, SPECT, multispectral, RGB, histology...\n",
        "\n",
        "In this tutorial, we will go through most of the transforms and show some usage examples, including a typical composition of transforms for training and testing.  \n",
        "Some important concepts of medical imaging are also explained, so it will be helpful for students, researchers or clinicians who are getting started in the field.\n",
        "\n",
        "If you are using Google Colab, you might find useful to navigate through the tutorial using the table of contents at the left of the screen.\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/AFRICAI-MICCAI/model_development_1_data/blob/main/Notebooks/4-5-%20Data-preprocessing-and-augmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlzHJtK2qzZo"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Conda environment\n",
        "\n",
        "It is suggested to create a conda environment for the summer school's notebooks. Please find conda installation instructions [here](https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html) (miniconda would be enough).  \n",
        "If you have not created/initialized the africai conda environment, run in a terminal from the parent  directory *model_development_1_data*:  \n",
        "> conda env create -f africai.yml  \n",
        "> conda activate africai\n",
        "\n",
        "*Other useful commands*:  \n",
        "To deactivate a conda environment \n",
        "> conda deactivate\n",
        "\n",
        "To delete a conda environment (e.g. africai conda environment, replace *ENV_NAME* with *africai*)\n",
        "> conda remove --name *ENV_NAME* --all "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install PyTorch  before installing TorchIO. It is recommended to use light-the-torch.  \n",
        "In a terminal, run:\n",
        "> pip install light-the-torch  \n",
        "> ltt install torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih08PH6Cs20c"
      },
      "source": [
        "Then [install TorchIO](https://torchio.readthedocs.io/quickstart.html#installation) and other libraries, download a couple of files and import the necessary modules."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "id": "g3HNQCtE6g6S",
        "outputId": "bbc5a262-f52d-4dc9-a8dc-563c5001f693"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "!pip install torchio==0.18.90 --quiet\n",
        "!pip install pandas --quiet\n",
        "!pip install matplotlib --quiet\n",
        "!pip install seaborn --quiet\n",
        "!pip install scikit-image --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import copy\n",
        "import time\n",
        "import pprint\n",
        "import os\n",
        "from pathlib import Path\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import torch\n",
        "import torchio as tio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nibabel as nib\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set()\n",
        "\n",
        "sns.set_style(\"whitegrid\", {'axes.grid' : False})\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "torch.manual_seed(14041931)\n",
        "\n",
        "print('TorchIO version:', tio.__version__)\n",
        "print('Last run on', time.ctime())\n",
        "\n",
        "data_dir = os.path.join(os.path.dirname(os.getcwd()), \"data\")\n",
        "Path(data_dir).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "!cd {data_dir} && curl -s -o colormap.txt https://raw.githubusercontent.com/thenineteen/Semiology-Visualisation-Tool/master/slicer/Resources/Color/BrainAnatomyLabelsV3_0.txt\n",
        "!cd {data_dir} && curl -s -o slice_7t.jpg https://www.statnews.com/wp-content/uploads/2019/08/x961_unsmoothed_cropped-copy-768x553.jpg\n",
        "!cd {data_dir} && curl -s -o slice_histo.jpg https://upload.wikimedia.org/wikipedia/commons/6/64/Medulloepithelioma_Histology.jpg\n",
        "!cd {data_dir} && curl -s -o vhp.zip https://data.lhncbc.nlm.nih.gov/public/Visible-Human/Sample-Data/Six%20slices%20from%20the%20Visible%20Male.zip\n",
        "\n",
        "file_name = os.path.join(data_dir, \"vhp.zip\")\n",
        "with ZipFile(file_name, \"r\") as zip:\n",
        "    zip.extractall(os.path.join(data_dir, \"vhp\"))\n",
        "    print(\"Input data file unzipped!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WjElZBhrsD-Y"
      },
      "source": [
        "Below are some visualization functions. The cell is hidden by default on Google Colab because the code is not directly related to this tutorial, but you can expand it by double-clicking on it, if you are curious."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "IA2LRIRc62Pr"
      },
      "outputs": [],
      "source": [
        "def get_bounds(self):\n",
        "    \"\"\"Get image bounds in mm.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: [description]\n",
        "    \"\"\"\n",
        "    first_index = 3 * (-0.5,)\n",
        "    last_index = np.array(self.spatial_shape) - 0.5\n",
        "    first_point = nib.affines.apply_affine(self.affine, first_index)\n",
        "    last_point = nib.affines.apply_affine(self.affine, last_index)\n",
        "    array = np.array((first_point, last_point))\n",
        "    bounds_x, bounds_y, bounds_z = array.T.tolist()\n",
        "    return bounds_x, bounds_y, bounds_z\n",
        "\n",
        "def to_pil(image):\n",
        "    from PIL import Image\n",
        "    from IPython.display import display\n",
        "    data = image.numpy().squeeze().T\n",
        "    data = data.astype(np.uint8)\n",
        "    image = Image.fromarray(data)\n",
        "    w, h = image.size\n",
        "    display(image)\n",
        "    print()  # in case multiple images are being displayed\n",
        "\n",
        "def stretch(img):\n",
        "    p1, p99 = np.percentile(img, (1, 99))\n",
        "    from skimage import exposure\n",
        "    img_rescale = exposure.rescale_intensity(img, in_range=(p1, p99))\n",
        "    return img_rescale\n",
        "\n",
        "def show_fpg(\n",
        "        subject,\n",
        "        to_ras=False,\n",
        "        stretch_slices=True,\n",
        "        indices=None,\n",
        "        intensity_name='t1',\n",
        "        parcellation=True,\n",
        "        ):\n",
        "    subject = tio.ToCanonical()(subject) if to_ras else subject\n",
        "    def flip(x):\n",
        "        return np.rot90(x)\n",
        "    fig, axes = plt.subplots(2, 3, figsize=(12, 8))\n",
        "    if indices is None:\n",
        "        half_shape = torch.Tensor(subject.spatial_shape) // 2\n",
        "        i, j, k = half_shape.long()\n",
        "        i -= 5  # use a better slice\n",
        "    else:\n",
        "        i, j, k = indices\n",
        "    bounds_x, bounds_y, bounds_z = get_bounds(subject.t1)  ###\n",
        "\n",
        "    orientation = ''.join(subject.t1.orientation)\n",
        "    if orientation != 'RAS':\n",
        "        import warnings\n",
        "        warnings.warn(f'Image orientation should be RAS+, not {orientation}+')\n",
        "    \n",
        "    kwargs = dict(cmap='gray', interpolation='none')\n",
        "    data = subject[intensity_name].data\n",
        "    slices = data[0, i], data[0, :, j], data[0, ..., k]\n",
        "    if stretch_slices:\n",
        "        slices = [stretch(s.numpy()) for s in slices]\n",
        "    sag, cor, axi = slices\n",
        "    \n",
        "    axes[0, 0].imshow(flip(sag), extent=bounds_y + bounds_z, **kwargs)\n",
        "    axes[0, 1].imshow(flip(cor), extent=bounds_x + bounds_z, **kwargs)\n",
        "    axes[0, 2].imshow(flip(axi), extent=bounds_x + bounds_y, **kwargs)\n",
        "\n",
        "    kwargs = dict(interpolation='none')\n",
        "    data = subject.seg.data\n",
        "    slices = data[0, i], data[0, :, j], data[0, ..., k]\n",
        "    if parcellation:\n",
        "        sag, cor, axi = [color_table.colorize(s.long()) if s.max() > 1 else s for s in slices]\n",
        "    else:\n",
        "        sag, cor, axi = slices\n",
        "    axes[1, 0].imshow(flip(sag), extent=bounds_y + bounds_z, **kwargs)\n",
        "    axes[1, 1].imshow(flip(cor), extent=bounds_x + bounds_z, **kwargs)\n",
        "    axes[1, 2].imshow(flip(axi), extent=bounds_x + bounds_y, **kwargs)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "\n",
        "\n",
        "class ColorTable:\n",
        "    def __init__(self, colors_path):\n",
        "        self.df = self.read_color_table(colors_path)\n",
        "\n",
        "    @staticmethod\n",
        "    def read_color_table(colors_path):\n",
        "        df = pd.read_csv(\n",
        "            colors_path,\n",
        "            sep=' ',\n",
        "            header=None,\n",
        "            names=['Label', 'Name', 'R', 'G', 'B', 'A'],\n",
        "            index_col='Label',\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    def get_color(self, label: int):\n",
        "        \"\"\"\n",
        "        There must be nicer ways of doing this\n",
        "        \"\"\"\n",
        "        try:\n",
        "            rgb = (\n",
        "                self.df.loc[label].R,\n",
        "                self.df.loc[label].G,\n",
        "                self.df.loc[label].B,\n",
        "            )\n",
        "        except KeyError:\n",
        "            rgb = 0, 0, 0\n",
        "        return rgb\n",
        "\n",
        "    def colorize(self, label_map: np.ndarray) -> np.ndarray:\n",
        "        rgb = np.stack(3 * [label_map], axis=-1)\n",
        "        for label in np.unique(label_map):\n",
        "            mask = label_map == label\n",
        "            color = self.get_color(label)\n",
        "            rgb[mask] = color\n",
        "        return rgb\n",
        "\n",
        "color_table = ColorTable(os.path.join(data_dir, 'colormap.txt'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqvyxV16top9"
      },
      "source": [
        "## Input types"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WoDCeh2YeGIp"
      },
      "source": [
        "In TorchIO, [transforms](https://torchio.readthedocs.io/transforms/transforms.html) input can be multiple things:\n",
        "1. TorchIO `Subject`\n",
        "2. TorchIO `Image`\n",
        "3. 4D NumPy array\n",
        "4. 4D PyTorch tensor\n",
        "5. SimpleITK image\n",
        "6. Python dictionary\n",
        "\n",
        "The output type will match the input type."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJxFbhx8upig"
      },
      "source": [
        "Let's try first a tiny 4D tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 247
        },
        "id": "xldAWoJZeML_",
        "outputId": "dca3fdcd-3218-48f4-8e94-243198cf9859"
      },
      "outputs": [],
      "source": [
        "add_noise = tio.RandomNoise()\n",
        "tensor = torch.ones(1, 2, 2, 2)\n",
        "transformed = add_noise(tensor)\n",
        "\n",
        "print('Before transform:')\n",
        "print(tensor)\n",
        "print()\n",
        "print('After transform:')\n",
        "print(transformed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMHpbYW1uu0Z"
      },
      "source": [
        "We can create a [`torchio.ScalarImage`]() from it, and the result will be of the same type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ffQakChPu1qY",
        "outputId": "1f9c4f92-07bc-4f31-c38a-664b4d5022e7"
      },
      "outputs": [],
      "source": [
        "image = tio.ScalarImage(tensor=tensor)\n",
        "transformed = add_noise(image)\n",
        "print(transformed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt6KSFDEvExl"
      },
      "source": [
        "Let's try with [`torchio.Subject`]()."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "azgrz2U1vLYY",
        "outputId": "06a0e087-1ace-4449-ced3-355b0e646fe2"
      },
      "outputs": [],
      "source": [
        "subject = tio.Subject(tiny=image)\n",
        "transformed = add_noise(subject)\n",
        "print(transformed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bpRvXWCpNYE"
      },
      "source": [
        "Python dictionary (mostly for compatibility with [Eisen](https://eisen.ai/) and [MONAI](https://monai.io/)). For this, we need to specify the keys when instantiating the transform:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "5QuotcQ5pcXt",
        "outputId": "2545fcf2-b7e0-447d-d645-7e34107e59f0"
      },
      "outputs": [],
      "source": [
        "python_dict = {'tiny': tensor}\n",
        "add_noise_dict = tio.RandomNoise(keys=['tiny'])\n",
        "transformed = add_noise_dict(python_dict)\n",
        "print(transformed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6rsKkqLvT78"
      },
      "source": [
        "And finally, a SimpleITK image:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "K6pp7ueEvTLx",
        "outputId": "8a575fe2-532d-4c97-e7f8-a03e12725280"
      },
      "outputs": [],
      "source": [
        "sitk_image = image.as_sitk()\n",
        "transformed = add_noise(sitk_image)\n",
        "print(type(transformed))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8d0_QmIgici"
      },
      "source": [
        "## Spatial transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wT5xSDFsnrd"
      },
      "source": [
        "Spatial transforms modify the image bounds or the voxel positions. They are applied to all images simultaneously, as we will se below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHYnuAkpq36p"
      },
      "source": [
        "We will use for the tutorial a dataset that can be downloaded with [`torchio.datasets`](https://torchio.readthedocs.io/datasets.html). It contains a 3T structural MRI, a corresponding [brain parcellation](http://niftyweb.cs.ucl.ac.uk/program.php?p=GIF) and two transforms to the [MNI space](https://www.lead-dbs.org/about-the-mni-spaces/), one rigid (6 degrees of freedom (DOF): 3 for rotation and 3 for translation) and one affine (12 DOF: 3 for rotation, 3 for translation, 3 for scaling and 3 for shearing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JzyScg4ZLmn9"
      },
      "source": [
        "Check out [NiBabel docs](https://nipy.org/nibabel/coordinate_systems.html) for an excellent explanation of affine matrices and coordinate systems in medical imaging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 911,
          "referenced_widgets": [
            "f6c7fa42d6a6440ea3e05de8137e7ef1",
            "45f50414f29a48eab863cefc24c65aa6",
            "5630f72c67604db48c0b920277647410",
            "c73bd4f297e14e5e908cdb03690755a0",
            "06d613aff63d47eeb6a28beca2543179",
            "f87e5763fc9649b48fef76e2b9fe10af",
            "a1439f7246de459ca27e0a6b767bc246",
            "cea8d173a62e476281269d613e3465a5",
            "2fc1286701664fe79db21984cd06bb10",
            "bd244e89db294129a257415fbde88358",
            "38a3e5e466dc46a48cac234a3351d72c",
            "bffd7a24102c4fbfb62695abc09ad5fb",
            "ffc7a28e60f446c99104ffe0e0195cc0",
            "510bcdd73bb24e9aa6814291e334d6fc",
            "14b4fe9ee6364c4eb58b72891a2b7542",
            "0e8c33484ac04b6481a8c8dcc1d33fe3",
            "7ed738f57b664fe098db464fbf27f764",
            "47beaaed3f4048e79a912ce069106eb7",
            "8ff0fd3038074956bc4f2f2a58bf9a06",
            "8277d2d1a78f4b99b43d1bf9221b4248",
            "32d5f64b8db74a008768948e0b709d5b",
            "27bd55bc73d94ddcbb619f829f5eca8e",
            "ff493385a6ec44a28b0710c933328477",
            "880f0d1f86a64341a93e413c1ac90025",
            "7adda7a46ef34f06a269c0f6e2297b48",
            "f0652d0485d74feaaa4e58ad61f8dcb0",
            "46e6cebf776f446e8873e635c5f22038",
            "dbc7baf2b24a4f6abfc9bf19822df4b5",
            "6b1ad5b0f27f48449d3a93855d3b97de",
            "17097ca557d049abb0f35e37e13a3314",
            "848f832eb09341dd8b33a2b8a7c0bc06",
            "5f677f52d7ab4412b356f8ccf953d93d"
          ]
        },
        "id": "Ah9tIA88mJjl",
        "outputId": "f63776e5-0383-4482-9361-f32da59a6456"
      },
      "outputs": [],
      "source": [
        "fpg = tio.datasets.FPG()\n",
        "print('Sample subject:', fpg)\n",
        "show_fpg(fpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fvvt1jzzwa7-"
      },
      "source": [
        "The aspect ratio is distorted and it looks upside down. This is because our visualization function expects images in [`RAS+` orientation](https://nipy.org/nibabel/image_orientation.html), the \"canonical\" orientation for the NIfTI format, but this image is in `PIR+`. We can use a preprocessing transform to fix that."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VthwFEOvkn3V"
      },
      "source": [
        "### Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6ewcWDSnq6c"
      },
      "source": [
        "#### To canonical"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpZlCh9gLspw"
      },
      "source": [
        "Medical images can be [oriented](https://www.slicer.org/wiki/Coordinate_systems) in many different ways. To ensure a consistent orientation across our dataset, we can use a preprocessing transform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_3j6irCepnW-"
      },
      "source": [
        "[`ToCanonical`](https://torchio.readthedocs.io/transforms/preprocessing.html#tocanonical) reorganizes the voxels positions and the associated [affine matrix](https://nipy.org/nibabel/coordinate_systems.html#the-affine-matrix-as-a-transformation-between-spaces) so that the image is now in `RAS+` orientation, i.e. the voxels indices will grow from left to right, from posterior (back) to anterior (nose) and from inferior (feet) to superior (head), respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "OEzQUT8ZnqBv",
        "outputId": "6d09a9d1-8a12-4c82-be0e-a5cbd840a243"
      },
      "outputs": [],
      "source": [
        "to_ras = tio.ToCanonical()\n",
        "fpg_ras = to_ras(fpg)\n",
        "print('Old orientation:', fpg.t1.orientation)\n",
        "print('New orientation:', fpg_ras.t1.orientation)\n",
        "show_fpg(fpg_ras)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAtglZSD_9v3"
      },
      "source": [
        "As you can see, the same transform is applied to the scalar image and to the label map."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "MYXKeEhqAE6S",
        "outputId": "a5df986e-25b0-4409-f39b-6914125c078c"
      },
      "outputs": [],
      "source": [
        "print(fpg_ras.t1)\n",
        "print(fpg_ras.seg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMKFOu3vpeph"
      },
      "source": [
        "#### Resample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YnHs9rhqCJ9"
      },
      "source": [
        "Images are discrete versions of continuous signals, therefore a spatial sampling frequency, and therefore also a period, are always associated with them. The spatial period is the distance between the voxel centers, or spacing. It is also the voxel size. We can check the image spacing (in mm) using the `spacing` property, or printing an image that has been loaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "L9oN6FDwphhz",
        "outputId": "f92871bb-1b7d-45ff-eb70-4709cfb9a443"
      },
      "outputs": [],
      "source": [
        "print(fpg_ras.t1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ksQJqvkqMZJ"
      },
      "source": [
        "We might prefer to resize our images to reduce computational burden. For that, we need to *downsample* our image. We can down- or up-sample our images with the [`Resample` transform](https://torchio.readthedocs.io/transforms/preprocessing.html#resample)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3uVJFF-8xN7"
      },
      "source": [
        "We will use an image with 3 times fewer voxels along each dimension than our original version. To keep the image bounds constant, the image spacing needs to be 3 times larger than the original one, i.e., 3 mm. Our final image will contain around $3^3$ times fewer voxels than the full-resolution one!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 657
        },
        "id": "nyumtwDeqLRa",
        "outputId": "c77107b8-a855-4914-ab45-107e07e8ef85"
      },
      "outputs": [],
      "source": [
        "downsampling_factor = 3\n",
        "original_spacing = 1\n",
        "target_spacing = downsampling_factor / original_spacing  # in mm\n",
        "downsample = tio.Resample(target_spacing)\n",
        "downsampled = downsample(fpg_ras)\n",
        "print('Original image:', fpg_ras.t1)\n",
        "print('Downsampled image:', downsampled.t1)\n",
        "print(f'The downsampled image takes {fpg_ras.t1.memory / downsampled.t1.memory:.1f} times less memory!')\n",
        "show_fpg(downsampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58toxMhkrHn7"
      },
      "source": [
        "The downsampling generates some [aliasing](https://en.wikipedia.org/wiki/Aliasing). We can use concepts of digital signal processing to compute an [optimal](https://link.springer.com/chapter/10.1007/978-3-319-24571-3_81) Gaussian kernel that we will convolve with the image to remove high frequencies before downsampling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "hgpjadsXqvkF",
        "outputId": "b7b0a7a2-ede4-4e43-be1d-022e97e7a7b0"
      },
      "outputs": [],
      "source": [
        "original_spacing = 1\n",
        "std = tio.Resample.get_sigma(downsampling_factor, original_spacing)\n",
        "antialiasing = tio.Blur(std)  # we need don't need a random transform here\n",
        "blurry = antialiasing(fpg_ras)\n",
        "downsampled_antialiasing = downsample(blurry)\n",
        "show_fpg(downsampled_antialiasing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_J3GX-M3Kmdx"
      },
      "source": [
        "This looks a bit better than the downsampled image without antialiasing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5_esashqLmG"
      },
      "source": [
        "Another handy use for the `Resample` transform is to apply a precomputed transformation to a standard space, such as the [MNI space](https://www.lead-dbs.org/about-the-mni-spaces/). The `FPG` dataset includes this transform:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "TpT2Y9nNApKJ",
        "outputId": "5a62cab4-f5be-43ec-aa6e-c22352c92983"
      },
      "outputs": [],
      "source": [
        "np.set_printoptions(precision=2, suppress=True)\n",
        "fpg.t1['affine_matrix'].numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxcHd6THBDA-"
      },
      "source": [
        "We can provide a reference image for the resampling. Let's download [a dataset in the MNI space](http://www.bic.mni.mcgill.ca/ServicesAtlases/Colin27) included in TorchIO and use it as reference space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 690,
          "referenced_widgets": [
            "abd24b80ac08421f90e3989d0af391de",
            "eb6f6cd6b1b64b2686bbe6ad0b51fc0c",
            "4be77144a81b457d9d48d0ce59ed7126",
            "cdaf1638077b4e38bb9f26fd336b25f8",
            "72c0bf0caecb4b49b781995f0d77c60d",
            "4ad2d4c433aa4e67a3ae9b9915020dd7",
            "458ff17be7ce46cc99871fafb294e7a6",
            "461ba75a59854150bf18a1b731bb35ca"
          ]
        },
        "id": "9F7Vikrer2Fn",
        "outputId": "36ebcf69-3a1f-412c-eedf-f928cbf88f2c"
      },
      "outputs": [],
      "source": [
        "mni = tio.datasets.Colin27()\n",
        "to_mni = tio.Resample(mni.t1.path, pre_affine_name='affine_matrix')\n",
        "fpg_mni = to_mni(fpg_ras)\n",
        "show_fpg(fpg_mni)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uPoMINBLIQA"
      },
      "source": [
        "That is our FPG dataset in the MNI space!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAmzE7wSjTKf"
      },
      "source": [
        "#### Crop or pad"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IMa4RVcLk-rq"
      },
      "source": [
        "Sometimes there is a large part of the image without much information. To reduce the computational burden, we will use [`CropOrPad`](https://torchio.readthedocs.io/transforms/preprocessing.html#croporpad) to crop the image around the brain. If the target shape is larger than the input shape for any dimension, the image will be padded instead along that dimension. We will use a (not helpful) large size along the lateral dimension to show this feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "sphJF4Wspvf2",
        "outputId": "b4de67ee-3e36-4410-c636-9f3c16047f58"
      },
      "outputs": [],
      "source": [
        "fpg_ras.t1.spatial_shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "as_fyMTYjOUf",
        "outputId": "3b00e1c5-d185-4147-acc3-268558d06daa"
      },
      "outputs": [],
      "source": [
        "target_shape = 300, 200, 170\n",
        "crop_pad = tio.CropOrPad(target_shape)\n",
        "show_fpg(crop_pad(fpg_ras))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3Sr5ssFC4Sv"
      },
      "source": [
        "If we don't know where the region of interest (ROI) is in the image, but have a segmentation of it, we can use it to perform the cropping/padding by setting the center of the image to be the center of the ROI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 526
        },
        "id": "1wtmcmfot7Do",
        "outputId": "1debe899-e50b-461e-cc69-36e6f5b06b0b"
      },
      "outputs": [],
      "source": [
        "target_shape = 300, 200, 170\n",
        "crop_pad = tio.CropOrPad(target_shape, mask_name='seg')\n",
        "show_fpg(crop_pad(fpg_ras))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnVCyKVq-Ug5"
      },
      "source": [
        "If our images are in a standard space, we probably have a good idea of the shape we need so that our image is large enough to fit our ROI but small enough for computations to be less expensive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "fdSdksKn-VBU",
        "outputId": "4cf355f5-f233-4cc1-9d42-aebfa4564513"
      },
      "outputs": [],
      "source": [
        "target_shape = 180, 220, 170\n",
        "crop_pad = tio.CropOrPad(target_shape)\n",
        "show_fpg(crop_pad(fpg_mni))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JPNQSLFGkumU"
      },
      "source": [
        "### Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k7aDr4K1lueN"
      },
      "source": [
        "#### Random downsample"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjCPrgbvlz7K"
      },
      "source": [
        "Many medical images are acquired with anisotropic spacing, i.e. voxels are not cubes. Researchers typically use anisotropic resampling for preprocessing before feeding the images into a neural network. We can simulate this effect downsampling our image along a specific dimension and resampling back to an isotropic spacing.\n",
        "\n",
        "To make the transform reproducible, we have passed a `seed` kwarg to the transform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 620
        },
        "id": "jGXtiujCltVb",
        "outputId": "c20b13ee-0be1-4240-bdf9-89db23455d26"
      },
      "outputs": [],
      "source": [
        "random_anisotropy = tio.RandomAnisotropy()\n",
        "fpg_anisotropic = random_anisotropy(fpg_mni)\n",
        "print('Applied transforms:')\n",
        "pprint.pprint(fpg_anisotropic.history)\n",
        "show_fpg(fpg_anisotropic)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxidVsB9Ms8c"
      },
      "source": [
        "Let's restore our image to its original resolution. Of course, this is a lossy operation, but that's the point! We want to increase the diversity of our dataset so that our models generalize better. Images in the real world have different artifacts and are not always isotropic, so this is a great transform for medical images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "c5D1OKLkx8n-",
        "outputId": "08e5a20f-c84b-4a8b-9693-cf46200ccd98"
      },
      "outputs": [],
      "source": [
        "resample_back = tio.Resample(fpg_mni.spacing)\n",
        "fpg_restored = resample_back(fpg_anisotropic)\n",
        "show_fpg(fpg_restored)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1h3H4arjpi5l"
      },
      "source": [
        "#### Random affine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bEGKlPcFESp"
      },
      "source": [
        "To simulate different positions and size of the patient within the scanner, we can use a [`RandomAffine`](https://torchio.readthedocs.io/transforms/augmentation.html#randomaffine) transform.\n",
        "\n",
        "To improve visualization, we will use a 2D image and add a grid to it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "pRNbPuPIr4qY",
        "outputId": "974c0ab4-e00d-47ca-c68b-e88bd3675000"
      },
      "outputs": [],
      "source": [
        "image = tio.ScalarImage(os.path.join(data_dir, 'slice_7t.jpg'))\n",
        "spacing = image.spacing[0]\n",
        "print('Downloaded slice:', image)\n",
        "image.as_pil()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "bpIeOyvetOZw",
        "outputId": "d9ba42b0-3823-4844-8664-75a6353c35de"
      },
      "outputs": [],
      "source": [
        "slice_grid = copy.deepcopy(image)\n",
        "data = slice_grid.data\n",
        "white = data.max()\n",
        "N = 25\n",
        "data[..., ::N, :, :] = white\n",
        "data[..., :, ::N, :] = white\n",
        "slice_grid.as_pil()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "HT_VbWkQvnF4",
        "outputId": "69fb6515-5591-4a4e-fc85-f41ef0d43202"
      },
      "outputs": [],
      "source": [
        "random_affine = tio.RandomAffine()\n",
        "slice_affine = random_affine(slice_grid)\n",
        "slice_affine.as_pil()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxDP4B32wCIZ"
      },
      "source": [
        "This transform can be used to zoom in or out, changing the `scales` kwarg."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "AjSZQlhTp_05",
        "outputId": "ec1807ba-89fd-4d02-ad87-f0063ec8de42"
      },
      "outputs": [],
      "source": [
        "random_affine_zoom = tio.RandomAffine(scales=(2, 2))\n",
        "slice_affine = random_affine_zoom(slice_grid)\n",
        "slice_affine.as_pil()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeFo7cbS6i9E"
      },
      "source": [
        "#### Random flip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QuNSbb806kkX"
      },
      "source": [
        "Flipping images is a very cheap way to perform data augmentation. In medical images, it's very common to flip the images horizontally. We can specify the dimensions indices when instantiating a [`RandomFlip`](https://torchio.readthedocs.io/transforms/augmentation.html#torchio.transforms.RandomFlip) transform.\n",
        "\n",
        "However, if we don't know the image orientation, we can't know which dimension corresponds to the lateral axis. In TorchIO, you can use anatomical labels instead, so that you don't need to figure out image orientation to know which axis you would like to flip.\n",
        "\n",
        "To make sure the transform modifies the image, we will use the inferior-superior (longitudinal) axis and a flip probability of 1. If the flipping happened along any other axis, we might not notice it using this visualization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "LOIW7BPN6lDC",
        "outputId": "5a6cf10b-f82b-47b0-93d0-3ead4ab9abc6"
      },
      "outputs": [],
      "source": [
        "random_flip = tio.RandomFlip(axes=['inferior-superior'], flip_probability=1)\n",
        "fpg_flipped = random_flip(fpg_ras)\n",
        "show_fpg(fpg_flipped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XckaqJS_wtZI"
      },
      "source": [
        "#### Random elastic deformation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Il0cNMlhwwLS"
      },
      "source": [
        "To simulate anatomical variations in our images, we can apply a non-linear deformation using [`RandomElasticDeformation`](https://torchio.readthedocs.io/transforms/augmentation.html#torchio.transforms.RandomElasticDeformation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "ESsBd55BwvPS",
        "outputId": "376e183d-9a05-43b4-cbcf-706b6d7885a8"
      },
      "outputs": [],
      "source": [
        "max_displacement = 15, 10, 0  # in x, y and z directions\n",
        "random_elastic = tio.RandomElasticDeformation(max_displacement=max_displacement)\n",
        "slice_elastic = random_elastic(slice_grid)\n",
        "slice_elastic.as_pil()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tUezsh_I7R6"
      },
      "source": [
        " As explained in the [documentation](https://torchio.readthedocs.io/transforms/augmentation.html#torchio.transforms.RandomElasticDeformation), one can change the number of grid control points to set the deformation smoothness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "sjFB-MxkxPRy",
        "outputId": "215376a0-523e-474a-84a8-f8e8057abb0f"
      },
      "outputs": [],
      "source": [
        "random_elastic = tio.RandomElasticDeformation(\n",
        "    max_displacement=max_displacement,\n",
        "    num_control_points=20,\n",
        ")\n",
        "slice_large_displacement_more_cp = random_elastic(slice_grid)\n",
        "slice_large_displacement_more_cp.as_pil()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2AXoau7Xk0p"
      },
      "source": [
        "As you can see, TorchIO has raised a warning explaining that our displacement is too large. If this happens, try using a smaller maximum displacement or fewer control points."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce3ume-jf9CV"
      },
      "source": [
        "Let's look at the effect of using few control points but very large displacements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "CtyfvmyUec5J",
        "outputId": "6860f0fc-2a60-41cd-bb0e-b55d228e3429"
      },
      "outputs": [],
      "source": [
        "random_elastic = tio.RandomElasticDeformation(\n",
        "    max_displacement=2 * np.array(max_displacement),\n",
        ")\n",
        "slice_large_displacement = random_elastic(slice_grid)\n",
        "slice_large_displacement.as_pil()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOivDmo7zF1T"
      },
      "source": [
        "## Intensity transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJZE2IbjoF7X"
      },
      "source": [
        "Intensity transforms modify only scalar images, whereas label maps are left as they were."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fR1CLvBLk8RX"
      },
      "source": [
        "### Preprocessing (normalization)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hT2YSWt6zPsS"
      },
      "source": [
        "#### Rescale intensity"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAQyd6udzSfw"
      },
      "source": [
        "We can change the intensities range of our images so that it lies within e.g. 0 and 1, or -1 and 1, using [`RescaleIntensity`](https://torchio.readthedocs.io/transforms/preprocessing.html#rescaleintensity)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "bGx-bGb4zRsE",
        "outputId": "ac6b6caf-05ca-4412-c919-36fc18994638"
      },
      "outputs": [],
      "source": [
        "rescale = tio.RescaleIntensity((-1, 1))\n",
        "rescaled = rescale(fpg)\n",
        "fig, axes = plt.subplots(2, 1)\n",
        "sns.distplot(fpg.t1.data, ax=axes[0], kde=False)\n",
        "sns.distplot(rescaled.t1.data, ax=axes[1], kde=False)\n",
        "axes[0].set_title('Original histogram')\n",
        "axes[1].set_title('Intensity rescaling')\n",
        "axes[0].set_ylim((0, 1e6))\n",
        "axes[1].set_ylim((0, 1e6))\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnYJ3BRxJk3x"
      },
      "source": [
        "There seem to be some outliers with very high intensity. We might be able to get rid of those by mapping some percentiles to our final values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        },
        "id": "oiZpLs5PJiUz",
        "outputId": "b8efefc8-a161-497e-8428-e43ed072fc82"
      },
      "outputs": [],
      "source": [
        "rescaled = rescale(fpg_ras)\n",
        "fig, axes = plt.subplots(2, 1)\n",
        "sns.distplot(fpg.t1.data, ax=axes[0], kde=False)\n",
        "sns.distplot(rescaled.t1.data, ax=axes[1], kde=False)\n",
        "axes[0].set_title('Original histogram')\n",
        "axes[1].set_title('Intensity rescaling with percentiles 1 and 99')\n",
        "axes[0].set_ylim((0, 1e6))\n",
        "axes[1].set_ylim((0, 1e6))\n",
        "plt.tight_layout()\n",
        "show_fpg(rescaled)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "biVIn4Yg1J-Q"
      },
      "source": [
        "#### Z-normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Kfn9qtm1Mqo"
      },
      "source": [
        "Another common approach for normalization is forcing data points to have zero-mean and unit variance. We can use [`ZNormalization`](https://torchio.readthedocs.io/transforms/preprocessing.html#znormalization) for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "qqZP3uxUzMiQ",
        "outputId": "bf223f7b-a719-4ce7-8f91-f5699269ec67"
      },
      "outputs": [],
      "source": [
        "standardize = tio.ZNormalization()\n",
        "standardized = standardize(fpg)\n",
        "fig, axes = plt.subplots(2, 1)\n",
        "sns.distplot(fpg.t1.data, ax=axes[0], kde=False)\n",
        "sns.distplot(standardized.t1.data, ax=axes[1], kde=False)\n",
        "axes[0].set_title('Original histogram')\n",
        "axes[1].set_title('Z-normalization')\n",
        "axes[0].set_ylim((0, 1e6))\n",
        "axes[1].set_ylim((0, 1e6))\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OewCa9N2BJD"
      },
      "source": [
        "The second mode in our distribution, corresponding to the foreground, is far from zero because the background contributes a lot to the mean computation. We can compute the stats using e.g. values above the mean only. Let's see if the mean is a good threshold to segment the foreground."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "soHZCsrw2iiw",
        "outputId": "85cd91a8-15f7-4512-d95b-65f0757b3211"
      },
      "outputs": [],
      "source": [
        "fpg_thresholded = copy.deepcopy(fpg_ras)\n",
        "data = fpg_thresholded.t1.data\n",
        "data[data > data.float().mean()] = data.max()\n",
        "show_fpg(fpg_thresholded)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioPwFprH2jLd"
      },
      "source": [
        "It seems reasonable to use this mask to compute the stats for our normalization transform."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "OxdmPKV62AlC",
        "outputId": "331626fc-8bb1-4926-b0b6-24378651c596"
      },
      "outputs": [],
      "source": [
        "standardize_foreground = tio.ZNormalization(masking_method=tio.ZNormalization.mean)\n",
        "standardized_foreground = standardize_foreground(fpg)\n",
        "fig, axes = plt.subplots(2, 1)\n",
        "sns.distplot(fpg.t1.data, ax=axes[0], kde=False)\n",
        "sns.distplot(standardized_foreground.t1.data, ax=axes[1], kde=False)\n",
        "axes[0].set_title('Original histogram')\n",
        "axes[1].set_title('Z-normalization using foreground stats')\n",
        "axes[0].set_ylim((0, 1e6))\n",
        "axes[1].set_ylim((0, 1e6))\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EnXGFl6hLYTv"
      },
      "source": [
        "The second mode is now closer to zero, as only the foreground voxels have been used to compute the statistics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rrVsjyBm3dw_"
      },
      "source": [
        "#### Histogram standardization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kam3e8sc3eUu"
      },
      "source": [
        "[Histogram standardization](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.204.102&rep=rep1&type=pdf) is yet another technique we can use to normalize our data. First, a set of landmarks is created from a training set. Then, images can be normalized by matching their histograms to the training landmarks.\n",
        "\n",
        "We will use landmarks derived from the [ADNI](http://adni.loni.usc.edu/) dataset, computed by [Li et al](https://arxiv.org/abs/1707.01992)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 864
        },
        "id": "85COw2H63PfH",
        "outputId": "e7fc0993-fa83-43a1-c647-634d1ec55b3b"
      },
      "outputs": [],
      "source": [
        "# From NiftyNet model zoo\n",
        "LI_LANDMARKS = \"0 8.06305571158 15.5085721044 18.7007018006 21.5032879029 26.1413278906 29.9862059045 33.8384058795 38.1891334787 40.7217966068 44.0109152758 58.3906435207 100.0\"\n",
        "LI_LANDMARKS = np.array([float(n) for n in LI_LANDMARKS.split()])\n",
        "landmarks_dict = {'t1': LI_LANDMARKS}\n",
        "hist_standardize = tio.HistogramStandardization(landmarks_dict, masking_method=tio.ZNormalization.mean)\n",
        "hist_standard = hist_standardize(fpg_ras)\n",
        "fig, axes = plt.subplots(2, 1)\n",
        "sns.distplot(fpg.t1.data, ax=axes[0], kde=False)\n",
        "sns.distplot(hist_standard.t1.data, ax=axes[1], kde=False)\n",
        "axes[0].set_title('Original histogram')\n",
        "axes[1].set_title('Histogram standardization using foreground stats')\n",
        "axes[0].set_ylim((0, 1e6))\n",
        "axes[1].set_ylim((0, 1e6))\n",
        "plt.tight_layout()\n",
        "show_fpg(hist_standard)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRion0gflC5M"
      },
      "source": [
        "### Augmentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmeuEVkl6V-p"
      },
      "source": [
        "#### Random blur"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u5iAvfVF8Xne"
      },
      "source": [
        "We can use [`RandomBlur`](https://torchio.readthedocs.io/transforms/augmentation.html#torchio.transforms.RandomBlur) to smooth/blur the images. The standard deviations of the Gaussian kernels are expressed in mm and will be computed independently for each axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "3sFPIxTa6W1M",
        "outputId": "b5e96fc0-5b1b-4946-f60d-84e433fbb4dc"
      },
      "outputs": [],
      "source": [
        "blur = tio.RandomBlur()\n",
        "blurred = blur(fpg_ras)\n",
        "show_fpg(blurred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KRaLonXa8YzE"
      },
      "source": [
        "#### Random noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2b-By2N8a2C"
      },
      "source": [
        "Gaussian noise can be simulated using [`RandomNoise`](https://torchio.readthedocs.io/transforms/augmentation.html#randomnoise). This transform is easiest to use after [`ZNormalization`](https://torchio.readthedocs.io/transforms/preprocessing.html#torchio.transforms.ZNormalization), as we know beforehand that the mean and standard deviation of the input will be 0 and 1, respectively. If necessary, the noise `mean` and `std` can be set using the corresponding keyword arguments.\n",
        "\n",
        "Noise in MRI is actually [Rician](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2254141/), but it is nearly Gaussian for SNR > 2 (i.e. foreground)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "VUiLh3rg8bEj",
        "outputId": "f957d324-1b76-48c7-ea85-da3c73378718"
      },
      "outputs": [],
      "source": [
        "add_noise = tio.RandomNoise(std=0.5)\n",
        "standard = standardize(fpg_ras)\n",
        "noisy = add_noise(standard)\n",
        "show_fpg(noisy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-NAM55p89Bm"
      },
      "source": [
        "#### MRI-specific transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqXG9EFZ8_re"
      },
      "source": [
        "TorchIO includes some transforms to simulate image artifacts specific to MRI modalities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PC8sMC5r9Aft"
      },
      "source": [
        "##### Random bias field"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-l-5agO9CzV"
      },
      "source": [
        "Magnetic field inhomogeneities in the MRI scanner produce low-frequency intensity distortions in the images, which are typically corrected using algorithms such as [N4ITK](https://pubmed.ncbi.nlm.nih.gov/20378467/). To simulate this artifact, we can use [`RandomBiasField`](https://torchio.readthedocs.io/transforms/augmentation.html#torchio.transforms.RandomBiasField).\n",
        "\n",
        "For this example, we will use an image that has been preprocessed so it's meant to be unbiased."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PaWwtsaQ9DBt",
        "outputId": "05122c89-a89c-4332-d29b-f95211661ffb"
      },
      "outputs": [],
      "source": [
        "add_bias = tio.RandomBiasField(coefficients=1)\n",
        "mni.seg = mni.brain\n",
        "show_fpg(mni)\n",
        "mni_bias = add_bias(mni)\n",
        "mni_bias.seg = mni_bias.brain\n",
        "show_fpg(mni_bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8dAoYSHTqh-"
      },
      "source": [
        "The artifact strength is proportional to the `coefficients` kwarg. The polynomial order is related to the artifact frequency, and the transform run time is proportional grows with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9YyFpV8qpQn"
      },
      "source": [
        "##### $k$-space transforms\n",
        "\n",
        "MR images are generated by computing the inverse Fourier transform of the $k$-space, which is the signal received by the coils in the scanner. If the $k$-space is altered, an artifact will be created in the image. These artifacts are typically accidental, but we can use transforms to simulate them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSfGlkqC9Pd1"
      },
      "source": [
        "##### Random spike"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJbtk50T9Rxh"
      },
      "source": [
        "Sometimes, signal peaks can appear in $k$-space. If one adds a high-energy component at e.g. 440 Hz in the spectrum of an audio signal, a tone of that frequency will be audible in the time domain. Similarly, spikes in $k$-space manifest as stripes in image space. They can be simulated using [`RandomSpike`](https://torchio.readthedocs.io/transforms/augmentation.html#torchio.transforms.RandomSpike). The number of spikes doesn't affect the transform run time, so try adding more!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "H55tLjZV9SOD",
        "outputId": "74c2b060-5b6a-4d22-d58d-556e8f21e8fc"
      },
      "outputs": [],
      "source": [
        "add_spike = tio.RandomSpike()\n",
        "with_spike = add_spike(fpg_ras)\n",
        "show_fpg(with_spike)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlmnV5Ea9Scc"
      },
      "source": [
        "##### Random ghosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uoUTBqtx9UAx"
      },
      "source": [
        "Ghosting artifacts, caused by patient motion, can be simulated by removing every $n$th plane from the k-space, and can be generated using [`RandomGhosting`](https://torchio.readthedocs.io/transforms/augmentation.html#randomghosting). As with the previous transform, the number of ghosts doesn't affect the run time, so you can add as many as you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "fzuyw2L79UMG",
        "outputId": "f0ba9908-2a2d-4a1b-d8c4-4816fba92ba3"
      },
      "outputs": [],
      "source": [
        "add_ghosts = tio.RandomGhosting(intensity=1.5)\n",
        "with_ghosts = add_ghosts(fpg_ras)\n",
        "show_fpg(with_ghosts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IC4d1q0E9Dqz"
      },
      "source": [
        "##### Random motion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SEft6Op9Oe7"
      },
      "source": [
        "If the patient moves during the MRI acquisition, motion artifacts will be present. TorchIO includes an implementation of [Shaw et al.](http://proceedings.mlr.press/v102/shaw19a.html), where the artifact is generated by filling the $k$-space with random rigidly-transformed versions of the original images and computing the inverse transform of the compound $k$-space.\n",
        "\n",
        "Computing the direct and inverse Fourier transform takes some time, so we'll use nearest neighbor interpolation to resample faster. Another way of cutting down the run time is using a smaller number of transforms (i.e., the patient moves less during acquisition time)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "MEYJ8mqg9O9S",
        "outputId": "3f18d2a8-2386-41f5-f428-11a4a7fd83c8"
      },
      "outputs": [],
      "source": [
        "add_motion = tio.RandomMotion(num_transforms=6, image_interpolation='nearest')\n",
        "with_motion = add_motion(fpg_ras)\n",
        "show_fpg(with_motion)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk6fxcUO9hKv"
      },
      "source": [
        "## Lambda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVMlf9uY9jbB"
      },
      "source": [
        "The fastest way to implement a custom transform is using [`Lambda`](https://torchio.readthedocs.io/transforms/others.html#lambda). This transform needs a callable object which applies a transform to a 4D image. We will use a histology slice to show a usage example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "7ixmjX9E5PbO",
        "outputId": "5150b220-161e-4499-ef7a-c8381e1e0e1f"
      },
      "outputs": [],
      "source": [
        "rgb = tio.ScalarImage(os.path.join(data_dir, 'slice_histo.jpg'))\n",
        "rgb.as_pil()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hxA0HIB05W19"
      },
      "source": [
        "We will pass a function that swaps the red and blue channels of the RGB image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        },
        "id": "MpaOcCDM9jyH",
        "outputId": "24301830-2f7b-49c2-8c20-0c6927db5dee"
      },
      "outputs": [],
      "source": [
        "def rgb_to_bgr(tensor):\n",
        "    tensor = torch.from_numpy(tensor.numpy()[::-1].copy())\n",
        "    return tensor\n",
        "\n",
        "transform = tio.Lambda(rgb_to_bgr)\n",
        "bgr = transform(rgb)\n",
        "bgr.as_pil()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1n13_ym9km7"
      },
      "source": [
        "## Composition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcvbYzhR9moD"
      },
      "source": [
        "It's useful to chain transforms together so that they can be applied with a single statement, instead of a `for` loop.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmH9rw_y9naK"
      },
      "source": [
        "### Compose"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY9Iuj5F9pik"
      },
      "source": [
        "Transforms can be easily chained together using [`Compose`](https://torchio.readthedocs.io/transforms/augmentation.html#torchio.transforms.Compose). It is like [`torchvision.Compose`](https://pytorch.org/docs/stable/torchvision/transforms.html#torchvision.transforms.Compose), but it takes a `p` kwarg representing the probability that the composition will be applied."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "isUXgAiY9p80",
        "outputId": "94531b28-cc72-4657-accb-8884cb680e30"
      },
      "outputs": [],
      "source": [
        "nice_downsample = tio.Compose([\n",
        "    to_ras,\n",
        "    antialiasing,\n",
        "    downsample,                   \n",
        "])\n",
        "nice_down = nice_downsample(fpg)\n",
        "show_fpg(nice_down)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbaJG76_9qeN"
      },
      "source": [
        "### \"One of\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeXBAAV_9tzQ"
      },
      "source": [
        "Sometimes we would like to apply e.g. either a `RandomAffine` or a `RandomElasticDeformation`, instead of both. However, computing a `RandomAffine` is faster. Therefore, we might want it to happen to only 25% of images, not 50%. We can use [`OneOf`](https://torchio.readthedocs.io/transforms/augmentation.html#oneof) for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZmrMw2k89uBt"
      },
      "outputs": [],
      "source": [
        "spatial_transform = tio.OneOf({\n",
        "    tio.RandomAffine(): 0.75,\n",
        "    tio.RandomElasticDeformation(): 0.25,\n",
        "})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZAU0fku9xJV"
      },
      "source": [
        "### Composition example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjKmdl_WtgHA"
      },
      "source": [
        "This an example of a possible composition of transforms used to train a convolutional neural network. Sometimes the order matters, sometimes it doesn't. Think deeply about the order of your transforms! For example, if you add noise with `RandomNoise` and then blur with `RandomBlur`, you'll be removing a lot of that noise!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo3MRK5995Iw"
      },
      "source": [
        "Do **not** use a seed for your random transforms during training! If you do, the same transform will be applied to every image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 655
        },
        "id": "A_92zltJ95Ug",
        "outputId": "0576205a-2b39-4f8c-cc9e-457426306493"
      },
      "outputs": [],
      "source": [
        "get_foreground = tio.ZNormalization.mean\n",
        "\n",
        "training_transform = tio.Compose([\n",
        "    tio.Resample(\n",
        "        mni.t1.path,\n",
        "        pre_affine_name='affine_matrix'),      # to MNI space (which is RAS+)\n",
        "    tio.RandomAnisotropy(p=0.25),              # make images look anisotropic 25% of times\n",
        "    tio.CropOrPad((180, 220, 170)),            # tight crop around brain\n",
        "    tio.HistogramStandardization(\n",
        "        landmarks_dict,\n",
        "        masking_method=get_foreground),        # standardize histogram of foreground\n",
        "    tio.ZNormalization(\n",
        "        masking_method=get_foreground),        # zero mean, unit variance of foreground\n",
        "    tio.RandomBlur(p=0.25),                    # blur 25% of times\n",
        "    tio.RandomNoise(p=0.25),                   # Gaussian noise 25% of times\n",
        "    tio.OneOf({                                # either\n",
        "        tio.RandomAffine(): 0.8,               # random affine\n",
        "        tio.RandomElasticDeformation(): 0.2,   # or random elastic deformation\n",
        "    }, p=0.8),                                 # applied to 80% of images\n",
        "    tio.RandomBiasField(p=0.3),                # magnetic field inhomogeneity 30% of times\n",
        "    tio.OneOf({                                # either\n",
        "        tio.RandomMotion(): 1,                 # random motion artifact\n",
        "        tio.RandomSpike(): 2,                  # or spikes\n",
        "        tio.RandomGhosting(): 2,               # or ghosts\n",
        "    }, p=0.5),                                 # applied to 50% of images\n",
        "])\n",
        "\n",
        "fpg_training = tio.datasets.FPG()\n",
        "fpg_augmented = training_transform(fpg_training)         # apply the transform\n",
        "show_fpg(fpg_augmented)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bh7Jcz7AEueH"
      },
      "source": [
        "Applying many transforms to large images can be slow. Tips to go faster:\n",
        "1. Use nearest neighbor interpolation\n",
        "2. Crop the volumes\n",
        "3. Downsample the images\n",
        "4. Use `OneOf` and the `p` kwarg wisely\n",
        "5. Use a small number of transforms in `RandomMotion`\n",
        "6. Use multiple workers in a `DataLoader`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1sa-T35BS1m"
      },
      "source": [
        "Which random transforms have been applied? Using what parameters? All the information is saved in the `history` attribute of [`Subject`](https://torchio.readthedocs.io/data/subject.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "esxcUXfeBTby",
        "outputId": "2d4b97a0-d5b3-473e-e406-0638125b2915"
      },
      "outputs": [],
      "source": [
        "pprint.pprint(fpg_augmented.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gdt-3XHUBiW0"
      },
      "source": [
        "At test time, we only need the preprocessing transforms:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "YOHNPneJBiwb",
        "outputId": "7f890ea9-c64b-445e-fbb6-1947159b1d35"
      },
      "outputs": [],
      "source": [
        "testing_transform = tio.Compose([\n",
        "    tio.Resample(mni.t1.path, pre_affine_name='affine_matrix'),                   # to MNI space (which is RAS+)\n",
        "    tio.HistogramStandardization(landmarks_dict, masking_method=get_foreground),  # standard histogram\n",
        "    tio.ZNormalization(masking_method=get_foreground),                            # zero mean and unit std\n",
        "])\n",
        "\n",
        "fpg_testing = testing_transform(fpg)\n",
        "show_fpg(fpg_testing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmhOyHux8bfC"
      },
      "source": [
        "## Implementations of recent papers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OnWaNKaa3m3f"
      },
      "source": [
        "TorchIO transforms can be written to implement more complex or higher-level operations. These are some examples of transforms that implement recent studies or have been used in them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wpk1ymDF8j7m"
      },
      "source": [
        "#### Random swap"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpxNfNfm8lhk"
      },
      "source": [
        "[Chen et al.](https://www.sciencedirect.com/science/article/abs/pii/S1361841518304699) randomly swapped small image patches to perform image restoration as a pretext task for self-supervised learning. This can be replicated using [`RandomSwap`](https://torchio.readthedocs.io/transforms/augmentation.html#randomswap). You'll want to use a smaller number of iterations for 2D images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "jQSLELNb8l1R",
        "outputId": "503f39cf-ca94-41e9-ee6e-f542b3d27484"
      },
      "outputs": [],
      "source": [
        "swap = tio.RandomSwap()\n",
        "swapped = swap(fpg_mni)\n",
        "show_fpg(swapped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGAi1n6F8mYk"
      },
      "source": [
        "#### Random labels to image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0nS6vhfs830K"
      },
      "source": [
        "[Billot et al.](https://github.com/BBillot/SynthSeg) generated images with different contrasts using only tissue segmentations, obtaining great results for modality-agnostic image segmentation. [`RandomLabelsToImage`](https://torchio.readthedocs.io/transforms/augmentation.html#randomlabelstoimage) can be used to generate this type of images.\n",
        "\n",
        "The authors recommend smoothing the image after, for which we can use `RandomBlur` with deterministic parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 743,
          "referenced_widgets": [
            "6e0e605ec1cd47a6bc6c9f7d818bc2b5",
            "79f5339891004919be9c0cde8cac532f",
            "80bb172b6b3b440e8b4308c712928a4d",
            "e0ec36bd428942fb832b39118723fee3",
            "f180e6a6aabe421c95cf686d7fefb399",
            "20693228512e49238ec84999e397cccf",
            "63c78b4039df466daa4a4b150e4a56bd",
            "45c088dfc6fc465c9b1f662e78c04e39"
          ]
        },
        "id": "U5TOIyphztUG",
        "outputId": "3c37a439-a9ce-4406-d224-9fc76a048577"
      },
      "outputs": [],
      "source": [
        "print('Downloading dataset...')\n",
        "large_colin = tio.datasets.Colin27(version=2008)\n",
        "new_colin = tio.Subject(t1=large_colin.t1, seg=large_colin.cls)  # ignore T2 and PD\n",
        "print('Loading data...')\n",
        "new_colin.load()  # load data here so that transform run time is not affected\n",
        "print('Resampling...')\n",
        "smaller_colin = tio.Resample(2, image_interpolation='nearest')(new_colin)\n",
        "print('Applying transform...')\n",
        "lab2im = tio.RandomLabelsToImage(label_key='seg', image_key='synthetic')\n",
        "lab2im_blur = tio.RandomBlur((1, 1))\n",
        "labels_image = lab2im(smaller_colin)\n",
        "labels_image.synthetic.data[labels_image.seg.data == 0] = 0  \n",
        "labels_image_smooth = lab2im_blur(labels_image)\n",
        "show_fpg(labels_image_smooth, intensity_name='synthetic', parcellation=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHS3xWeN841U"
      },
      "source": [
        "#### Random resection (working better on Colab)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5kNE0kI86_F"
      },
      "source": [
        "[Prez-Garca et al.](https://arxiv.org/abs/2006.15693) simulated brain resection cavities from healthy subjects to pre-train a segmentation model using self-supervised learning. We will first download and install the [`resector`](https://github.com/fepegar/resector) package."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "H1UsseIy87SB",
        "outputId": "40e62611-830f-4c1b-c432-53181b2c2b97"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet resector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuLdHEjOOBbl"
      },
      "source": [
        "Now we can use the `resect` command to apply to the transform. The first time it is run, it will take some time as it needs to generate some files necessary to simulate the resection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 16
        },
        "id": "D8gJKrLPN312",
        "outputId": "06ea9182-1c4c-4646-a712-ba1567599fc0"
      },
      "outputs": [],
      "source": [
        "fpg_mni.t1.save(os.path.join(data_dir, 't1.nii.gz'))\n",
        "fpg_mni.seg.save(os.path.join('gif_parcellation.nii.gz'))\n",
        "!resect t1.nii.gz gif_parcellation.nii.gz t1_resected.nii.gz t1_resection_label.nii.gz --seed 42"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "NcfT_HGzOSuC",
        "outputId": "29cd437a-5aa2-4c87-e61f-d881a6d6cd67"
      },
      "outputs": [],
      "source": [
        "resected = tio.Subject(\n",
        "    t1=tio.ScalarImage(os.path.join(data_dir, 't1_resected.nii.gz')),\n",
        "    seg=tio.LabelMap(os.path.join(data_dir, 't1_resection_label.nii.gz'))\n",
        ")\n",
        "centroid = np.array(np.where(resected.seg.data)).mean(axis=1).astype(np.uint16)[-3:]\n",
        "show_fpg(resected, indices=centroid)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shxm3I5nCVZ9"
      },
      "source": [
        "## Other interfaces\n",
        "\n",
        "You can still try out the transforms using a GUI or a CLI tool even without using Python."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1n9zCh7CYWg"
      },
      "source": [
        "### 3D Slicer\n",
        "\n",
        "[3D Slicer](https://www.slicer.org/) is *an open source software platform for medical image informatics, image processing, and three-dimensional visualization*.\n",
        "\n",
        "The [extensions index](https://www.slicer.org/wiki/Documentation/Nightly/SlicerApplication/ExtensionsManager) include the [TorchIO](https://torchio.readthedocs.io/slicer.html) extension, that can be used to try out the transforms and their parameters.\n",
        "\n",
        "![TorchIO extension screenshot](https://raw.githubusercontent.com/fepegar/SlicerTorchIO/master/Screenshots/TorchIO.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HbVzBLoYCZzt"
      },
      "source": [
        "### Command-line tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS3-IWW7hBw3"
      },
      "source": [
        "After installing TorchIO with `pip`, you can just run the [`torchio-transform`](https://torchio.readthedocs.io/cli.html) command to apply a transform to a image file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQJkrRRJS-IZ"
      },
      "outputs": [],
      "source": [
        "fpg_mni.t1.save(os.path.join(data_dir, 't1.nii.gz'))\n",
        "!tiotr --kwargs \"degrees=20 scales=(0.5,2)\" --seed 42 ../data/t1.nii.gz RandomAffine ../data/t1_cli.nii.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also use tiohd to check the header and plot the image, although plotting won't work on Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!tiohd --plot ../data/t1_cli.nii.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CqhDS8ICNH2"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "Preprocessing and augmentation of medical images are important but not trivial steps in all deep learning pipelines. In this tutorial, we have seen that [TorchIO transforms](https://torchio.readthedocs.io/transforms/transforms.html) can be used to perform these tasks in a simple, reproducible way, without reinventing the wheel.\n",
        "\n",
        "If you want to give feedback, as for features, report bugs or contribute, feel free to [open an issue](https://github.com/fepegar/torchio/issues/new/choose) on [GitHub](https://github.com/fepegar/torchio/)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmcc8p6Jm29-"
      },
      "source": [
        "## Exercise\n",
        "\n",
        "1. Use a slice from the [Visible Human Project](https://www.nlm.nih.gov/research/visible/visible_human.html) (VHP),instantiated in the cell below\n",
        "2. Flip it in the `'anteroposterior'` axis (you'll need to set a kwarg so that flipping happens always)\n",
        "2. Flip it in the `'lateral'` axis with a probability of 0.5\n",
        "2. Crop the image around the center using a target shape of `(800, 800, 1)`\n",
        "3. Resample to 0.75 mm isotropic\n",
        "4. Perform an elastic deformation using a maximum displacement of 50 mm **or** an affine transformation, with a 60% probability that the elastic transformation will be chosen\n",
        "5. Blur the image with a probability of 0.6\n",
        "6. Add Gaussian noise with mean $\\mu = 128$ and standard deviation $\\sigma = 10$ and rescale the image intensity to the $[0, 255]$ range to avoid overflow artifacts when displaying the image **if** Gaussian noise was added\n",
        "8. Create a transform to convert the RGB tensor to grayscale*\n",
        "9. Concatenate all these transforms\n",
        "10. Apply the concatenated transform to the VHP slice 10 times and display the results using `to_pil`\n",
        "\n",
        "*Although this should be done [carefully](https://en.wikipedia.org/wiki/Grayscale#Colorimetric_(perceptual_luminance-preserving)_conversion_to_grayscale), an easy way is using the mean of the three (RGB) channels. Remember channels are stored in the first dimension in TorchIO. Tip: use `keepdim=True` in `torch.mean`.\n",
        "\n",
        "**Bonus**\n",
        "\n",
        "- Apply the transforms one by one, showing each transform name (`transform.name`) and run time. Print also the total run time. Finally, display each transformed image\n",
        "- Remove the cropping and resampling transforms and measure the times again. Are they different? Why?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Data preprocessing and augmentation using TorchIO - a tutorial.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06d613aff63d47eeb6a28beca2543179": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "0e8c33484ac04b6481a8c8dcc1d33fe3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14b4fe9ee6364c4eb58b72891a2b7542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17097ca557d049abb0f35e37e13a3314": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20693228512e49238ec84999e397cccf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27bd55bc73d94ddcbb619f829f5eca8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fc1286701664fe79db21984cd06bb10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38a3e5e466dc46a48cac234a3351d72c",
              "IPY_MODEL_bffd7a24102c4fbfb62695abc09ad5fb"
            ],
            "layout": "IPY_MODEL_bd244e89db294129a257415fbde88358"
          }
        },
        "32d5f64b8db74a008768948e0b709d5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "38a3e5e466dc46a48cac234a3351d72c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_510bcdd73bb24e9aa6814291e334d6fc",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ffc7a28e60f446c99104ffe0e0195cc0",
            "value": 1
          }
        },
        "458ff17be7ce46cc99871fafb294e7a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45c088dfc6fc465c9b1f662e78c04e39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45f50414f29a48eab863cefc24c65aa6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "461ba75a59854150bf18a1b731bb35ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46e6cebf776f446e8873e635c5f22038": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17097ca557d049abb0f35e37e13a3314",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b1ad5b0f27f48449d3a93855d3b97de",
            "value": 1
          }
        },
        "47beaaed3f4048e79a912ce069106eb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ad2d4c433aa4e67a3ae9b9915020dd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4be77144a81b457d9d48d0ce59ed7126": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4ad2d4c433aa4e67a3ae9b9915020dd7",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_72c0bf0caecb4b49b781995f0d77c60d",
            "value": 1
          }
        },
        "510bcdd73bb24e9aa6814291e334d6fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5630f72c67604db48c0b920277647410": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f87e5763fc9649b48fef76e2b9fe10af",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_06d613aff63d47eeb6a28beca2543179",
            "value": 1
          }
        },
        "5f677f52d7ab4412b356f8ccf953d93d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63c78b4039df466daa4a4b150e4a56bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b1ad5b0f27f48449d3a93855d3b97de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "6e0e605ec1cd47a6bc6c9f7d818bc2b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80bb172b6b3b440e8b4308c712928a4d",
              "IPY_MODEL_e0ec36bd428942fb832b39118723fee3"
            ],
            "layout": "IPY_MODEL_79f5339891004919be9c0cde8cac532f"
          }
        },
        "72c0bf0caecb4b49b781995f0d77c60d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "79f5339891004919be9c0cde8cac532f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7adda7a46ef34f06a269c0f6e2297b48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46e6cebf776f446e8873e635c5f22038",
              "IPY_MODEL_dbc7baf2b24a4f6abfc9bf19822df4b5"
            ],
            "layout": "IPY_MODEL_f0652d0485d74feaaa4e58ad61f8dcb0"
          }
        },
        "7ed738f57b664fe098db464fbf27f764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8ff0fd3038074956bc4f2f2a58bf9a06",
              "IPY_MODEL_8277d2d1a78f4b99b43d1bf9221b4248"
            ],
            "layout": "IPY_MODEL_47beaaed3f4048e79a912ce069106eb7"
          }
        },
        "80bb172b6b3b440e8b4308c712928a4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20693228512e49238ec84999e397cccf",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f180e6a6aabe421c95cf686d7fefb399",
            "value": 1
          }
        },
        "8277d2d1a78f4b99b43d1bf9221b4248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_880f0d1f86a64341a93e413c1ac90025",
            "placeholder": "",
            "style": "IPY_MODEL_ff493385a6ec44a28b0710c933328477",
            "value": " 8192/? [00:02&lt;00:00, 2766.22it/s]"
          }
        },
        "848f832eb09341dd8b33a2b8a7c0bc06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "880f0d1f86a64341a93e413c1ac90025": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ff0fd3038074956bc4f2f2a58bf9a06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27bd55bc73d94ddcbb619f829f5eca8e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32d5f64b8db74a008768948e0b709d5b",
            "value": 1
          }
        },
        "a1439f7246de459ca27e0a6b767bc246": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "abd24b80ac08421f90e3989d0af391de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4be77144a81b457d9d48d0ce59ed7126",
              "IPY_MODEL_cdaf1638077b4e38bb9f26fd336b25f8"
            ],
            "layout": "IPY_MODEL_eb6f6cd6b1b64b2686bbe6ad0b51fc0c"
          }
        },
        "bd244e89db294129a257415fbde88358": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bffd7a24102c4fbfb62695abc09ad5fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e8c33484ac04b6481a8c8dcc1d33fe3",
            "placeholder": "",
            "style": "IPY_MODEL_14b4fe9ee6364c4eb58b72891a2b7542",
            "value": " 778240/? [00:04&lt;00:00, 188466.44it/s]"
          }
        },
        "c73bd4f297e14e5e908cdb03690755a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cea8d173a62e476281269d613e3465a5",
            "placeholder": "",
            "style": "IPY_MODEL_a1439f7246de459ca27e0a6b767bc246",
            "value": " 10862592/? [00:06&lt;00:00, 1786289.31it/s]"
          }
        },
        "cdaf1638077b4e38bb9f26fd336b25f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_461ba75a59854150bf18a1b731bb35ca",
            "placeholder": "",
            "style": "IPY_MODEL_458ff17be7ce46cc99871fafb294e7a6",
            "value": " 24256512/? [00:05&lt;00:00, 4174074.72it/s]"
          }
        },
        "cea8d173a62e476281269d613e3465a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dbc7baf2b24a4f6abfc9bf19822df4b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f677f52d7ab4412b356f8ccf953d93d",
            "placeholder": "",
            "style": "IPY_MODEL_848f832eb09341dd8b33a2b8a7c0bc06",
            "value": " 16384/? [00:01&lt;00:00, 13246.22it/s]"
          }
        },
        "e0ec36bd428942fb832b39118723fee3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45c088dfc6fc465c9b1f662e78c04e39",
            "placeholder": "",
            "style": "IPY_MODEL_63c78b4039df466daa4a4b150e4a56bd",
            "value": " 304160768/? [00:41&lt;00:00, 15869910.53it/s]"
          }
        },
        "eb6f6cd6b1b64b2686bbe6ad0b51fc0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0652d0485d74feaaa4e58ad61f8dcb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f180e6a6aabe421c95cf686d7fefb399": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "f6c7fa42d6a6440ea3e05de8137e7ef1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5630f72c67604db48c0b920277647410",
              "IPY_MODEL_c73bd4f297e14e5e908cdb03690755a0"
            ],
            "layout": "IPY_MODEL_45f50414f29a48eab863cefc24c65aa6"
          }
        },
        "f87e5763fc9649b48fef76e2b9fe10af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff493385a6ec44a28b0710c933328477": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ffc7a28e60f446c99104ffe0e0195cc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
